{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a427d3-2d94-41ba-ae34-1ae47dff1926",
   "metadata": {},
   "source": [
    "## Entropy and information (Kardar 2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2651e-9e9f-4a8f-a180-3ca404b889e0",
   "metadata": {},
   "source": [
    "Previously, $S \\equiv \\ln [\\underbrace{\\# \\text { of configurations }}_{\\Omega}](*)$\n",
    "\n",
    "Example: given $N_{+}, \\Omega\\left(N_{+}\\right)=\\left(\\begin{array}{c}N \\\\ N_{+}\\end{array}\\right)=\\frac{N !}{N_{+} N_{-}}$\n",
    "\n",
    "$$\n",
    "\\text { or } S\\left(N_{+}\\right)=N_{+} \\ln N_{+} / N+N_{-} \\ln N_{-} / N\n",
    "$$\n",
    "\n",
    "$(*)$ is appropriate if all configurations are equally likely, which is the case if we fix $N_{+}$\n",
    "\n",
    "In general $N_+$ is not fixed but itself a random variable $\\rightarrow P_{S}(S) d S=P\\left(N_{+}\\right) d N_{+} $.\n",
    "\n",
    "But in the thermodynamic limit, we know\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\begin{array}{c}\n",
    "N_{+} \\rightarrow\\left\\langle N_{+}\\right\\rangle=p N, N_{-}=\\left\\langle N_{r}\\right\\rangle=q N . \\\\\n",
    "\\Rightarrow S=-N \\cdot(p \\ln p+q \\ln q) .\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\Rightarrow$ In the thermodynamic limit $(N \\rightarrow \\infty)$, we can only observe \"typical\" configurations $\\left(N_{+}=p N ; N_{-}=q N\\right)$; there are $e^{S}$ of them and all of them are equally likely $P({\\sigma_i})=p^{N} \\cdot q^{N}$ )\n",
    "\n",
    "These observations are easily generalized to a dice with M faces. If rolling the dice results in face $i$ with probability $p_{i}$, we expect face $i$ to show up exactly $N p_i$ times in the thermodynamic limit, $N \\rightarrow \\infty$. The number of typical configurations is therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Omega & \\equiv\\text{nr. of config's}=\\frac{N !}{\\left(N p_{1}\\right) !\\left(N p_{2}\\right) ! \\ldots\\left(N p_{n}\\right) !} \\\\\n",
    "& S\\equiv \\log_{2}(\\Omega)=N[\\log (N)-1]-\\sum_{i}\\left(N p_{i}\\right)\\left[\\log N p_{i}-1\\right] \\\\\n",
    "& =-N \\sum_{i=1} p_{i} \\log_2 p_{i}.\\left({*}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In physics, $(*)$ arises as the entropy change when $M$ components are mixed together. It is therefore called \"entropy of mixing\" (also closely related to Gibbs entropy).\n",
    "\n",
    "#### Information entropy\n",
    "\n",
    "Shannon realized that the number of possible configurations consistent with our macroscopic constraints can be viewed as a *lack of knowledge* about the current microstate.\n",
    "\n",
    "Examples: \n",
    "\n",
    "- Suppose we flip coin $N$ times and we know $N_+$. Then, the actual microstate is one of $e^{S\\left(N_{+}\\right)}$ micro-states.\n",
    "- If we don't know $N_+$, respectively $N_+$ is not fixed? $\\Rightarrow e^{S}$ typical microstates, $S=-N \\sum_{i} p_{i} \\log p_{i}$. For a coin: $S=-N (p \\log p +q \\log q)$.\n",
    "\n",
    "\n",
    "##### Consequences:\n",
    "\n",
    "Suppose we end up measuring the micro-state, how many bits do we need to store this information?\n",
    "\n",
    "For $N \\rightarrow \\infty$, simply enumerate only the $e^{S}$ typical microstates, all having came probabilities. This needs $\\log _{2}\\left(e^{s}\\right)=S \\cdot \\log _{2}(e)$ bits. (of course, this is not a proof, but it works because of CLT induced measure concentration.)\n",
    "\n",
    "To simplify notation Shannon introduced the information entropy, which for any probability distribution $p_{i}$, is defined as\n",
    "\n",
    "$$\n",
    "s(\\{p\\})=-\\left\\langle\\log p_{i}\\right\\rangle=-\\sum_{i} p_{i} \\log p_{i} \\;.\n",
    "$$\n",
    "\n",
    "$s(\\{p\\})$ represents the average bits per word needed to encode a long message of words $i=1 \\ldots M$ drawn form $p$.\n",
    "\n",
    "Note: \n",
    "- $s=\\log (M)$ if $p_{i}=$ cont. $=\\frac{1}{M} \\quad$ \"naive encoding\"\n",
    "- But $s<\\log (M)$ for any non-uniform probability distribution.\n",
    "- $I\\left[\\left\\{p_{i}\\right\\}\\right]=\\log_{2} (M)-S$ measures information content of the pdf.\n",
    "\n",
    "#### Estimation:\n",
    "\n",
    "Suppose we want to estimate a distribution of $X$, about which we have some partial information, e.g. we know the value of $\\langle X\\rangle=\\sum_{i} p_{i} X_{i}$ or $\\operatorname{var}(X)$, but not $\\left\\{p_{i}\\right\\}$.\n",
    "\n",
    "Then: Least biased probability distribution.is the one that maximises $s$ given constraints. This is called the MaxEnt approach.\n",
    "\n",
    "Example: \n",
    "\n",
    "Find MaxEnt distribution under the constraint of a given fixed calue $\\phi$ of $\\langle F(x)\\rangle=\\sum_{i} p_{i} F\\left(x_{i}\\right)$. We always have to ensure the constraint that the probability distribution sums up to one, $\\langle 1\\rangle=\\sum_{i} p_{i}=1$ always to ne.\n",
    "\n",
    "To maximize the entropy $s=-\\sum_{i} p_{i} \\log p_{i}$ subject to both constraint, we use two Lagrange multipliers $\\alpha, \\beta$ and maximize\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S^{*}\\left(\\alpha, \\beta,\\left\\{p_{i}\\right\\}\\right) & =-\\sum_{i} p_{i} \\ln p_{i}-\\alpha\\left(\\sum_{i} p_{i}-1\\right)- \\\\\n",
    "& -\\beta\\left(\\sum_{i} p_{i} F\\left(x_{i}\\right)-f\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\delta S^{*}}{\\partial p_{i}} & =-\\ln p_{i}^{*}-1-\\alpha-\\beta F\\left(x_{i}\\right) \\\\\n",
    "& \\Rightarrow p_{i}^{*}=e^{-\\left(1+\\alpha+\\beta F\\left(x_{i}\\right)\\right.}=\\frac{1}{Z} e^{-\\beta F\\left(x_{i}\\right)}\\;\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is familiar from the Boltzmann distrib.\n",
    "\n",
    "$$\n",
    "\\left(\\beta=\\frac{1}{k_{B} T} ; F=\\text { Enasy }\\right)\n",
    "$$\n",
    "\n",
    "$\\Rightarrow$ Boltzmann = Max-Ent subject to $\\langle H\\rangle=E$.\n",
    "\n",
    "Note: \n",
    "- This does not mean $p$ **is** $p^{*}$. Multiple $\\{p_i\\}$ may give the same $\\langle F(x)\\rangle$\n",
    "- One can add further constaints and update in light of extra knouledge.\n",
    "\n",
    "$$\n",
    "\\rightarrow p_{i} \\propto e^{-\\beta F_{1}(x)-\\gamma F_{2}(x)-\\ldots}\n",
    "$$\n",
    "- How does this compare to Bayes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a0c75-1a0b-47b4-8248-048d1effad9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
